# modules.py

This module provides utility functions and foundational neural blocks used by the tracking head:
- Tuple conversion helper (`_ntuple`, `to_2tuple`)
- Predicate helpers (`exists`, `default`)
- Convolutional residual block (`ResidualBlock`)
- Feed-forward MLP (`Mlp`)
- Self-attention block (`AttnBlock`)
- Cross-attention block (`CrossAttnBlock`)

## Functions

### `_ntuple(n)`
Factory for parsing an input into an n-tuple. If the input is iterable (and not a string), returns it as a tuple; otherwise repeats the value `n` times.

**Args:**
- `n` (`int`): Number of tuple elements.

**Returns:**
Callable that converts a value to an `n`-tuple.

### `exists(val)`
Simple predicate checking that `val` is not `None`.

**Args:**
- `val`: Any value.

**Returns:**
`True` if `val is not None`, else `False`.

### `default(val, d)`
Returns `val` if it exists, otherwise returns default `d`.

**Args:**
- `val`: Value to test.
- `d`: Default value or callable.

**Returns:**
`val` if `val is not None`, else `d`.

### `to_2tuple`
Alias for `_ntuple(2)`, converting a value into a 2-tuple.

## Classes

### `ResidualBlock`
**Description:**  
Residual block with two convolutional layers, normalization, and ReLU activations. Optionally downsamples the residual path when `stride != 1`.

**Constructor Args:**
- `in_planes` (`int`): Number of input channels.
- `planes` (`int`): Number of output channels.
- `norm_fn` (`str`): Normalization type (`"group"`, `"batch"`, `"instance"`, or `"none"`).
- `stride` (`int`): Stride for the first convolution (used for downsampling).
- `kernel_size` (`int`): Convolution kernel size (default: 3).

**Forward:**
1. Apply Conv2d → Norm → ReLU.
2. Apply Conv2d → Norm → ReLU.
3. Downsample input if needed.
4. Add residual and apply ReLU.

### `Mlp`
**Description:**  
Feed-forward network used in transformer and mixer architectures. Supports optional convolutional implementation.

**Constructor Args:**
- `in_features` (`int`): Input feature dimension.
- `hidden_features` (`int`, optional): Hidden layer dimension (defaults to `in_features`).
- `out_features` (`int`, optional): Output dimension (defaults to `in_features`).
- `act_layer` (`Callable`): Activation layer (default: `nn.GELU`).
- `norm_layer`: Unused normalization layer argument for compatibility.
- `bias` (`bool` or tuple): Bias for each linear/conv layer.
- `drop` (`float` or tuple): Dropout probability after each layer.
- `use_conv` (`bool`): If `True`, uses 1×1 Conv2d instead of Linear.

**Forward:**
1. Project `in_features` → `hidden_features`.
2. Apply activation and dropout.
3. Project `hidden_features` → `out_features`.
4. Apply dropout.

### `AttnBlock`
**Description:**  
Self-attention block with pre-norm, residual connections, and a feed-forward sublayer.

**Constructor Args:**
- `hidden_size` (`int`): Token embedding dimension.
- `num_heads` (`int`): Number of attention heads.
- `attn_class` (`Callable`): Attention implementation (default: `nn.MultiheadAttention`).
- `mlp_ratio` (`float`): Hidden-to-embedding ratio for the MLP.
- Additional keyword args passed to the attention class.

**Forward Args:**
- `x` (`Tensor[B, T, D]`): Input token sequence.
- `mask` (optional): Attention mask.

**Forward:**
1. LayerNorm → Self-attention.
2. Residual add.
3. LayerNorm → MLP.
4. Residual add.

### `CrossAttnBlock`
**Description:**  
Cross-attention block attending from query tokens to context tokens, followed by a feed-forward MLP.

**Constructor Args:**
- `hidden_size` (`int`): Query embedding dimension.
- `context_dim` (`int`): Context token embedding dimension (unused, kept for compatibility).
- `num_heads` (`int`): Number of attention heads.
- `mlp_ratio` (`float`): MLP hidden ratio.
- Additional keyword args passed to `nn.MultiheadAttention`.

**Forward Args:**
- `x` (`Tensor[B, T, D]`): Query tokens.
- `context` (`Tensor[B, S, D]`): Context tokens.
- `mask` (optional): Attention mask.

**Forward:**
1. LayerNorm on `x` and `context`.
2. MultiheadAttention(`x`, `context`, `context`).
3. Residual and MLP update.

*Generated by developer documentation script.*