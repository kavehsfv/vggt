# attention.py

This module implements multi-head self-attention layers with optional rotary embeddings and memory-efficient variants.

## Classes

### `Attention`

**Description:**  
Standard multi-head self-attention that:
1. Projects input tokens to Q, K, V.
2. Optionally normalizes Q and K.
3. Applies Rotary Position Embedding (`rope`) if provided.
4. Performs attention using fused `scaled_dot_product_attention` or manual dot-product + softmax.
5. Projects and drops out the output.

**Constructor Args:**  
- `dim` (`int`): Embedding dimension.  
- `num_heads` (`int`): Number of attention heads.  
- `qkv_bias`, `proj_bias` (`bool`): Enable biases.  
- `attn_drop`, `proj_drop` (`float`): Dropout rates.  
- `qk_norm` (`bool`): Normalize Q and K.  
- `fused_attn` (`bool`): Use optimized fused attention.  
- `rope`: Optional `RotaryPositionEmbedding2D`.

### `MemEffAttention`

**Description:**  
Extends `Attention` to use a block-diagonal attention bias and memory-efficient attention kernel (e.g., xFormers), enabling nested inputs via `NestedTensorBlock`.

*Generated by developer documentation script.*