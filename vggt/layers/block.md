# block.py

This module implements the core Transformer Block used throughout VGGT.
Each `Block` consists of a multi-head self-attention sublayer and a feed-forward (MLP) sublayer,
both wrapped with Layer Normalization, optional LayerScale, and stochastic depth (DropPath).
An extended variant, `NestedTensorBlock`, supports nested list inputs for memory-efficient attention.

## Classes

### `Block`

**Description:**
A standard transformer block that applies LayerNorm, multi-head self-attention (with optional RoPE),
followed by a residual connection with optional LayerScale and DropPath, then a second LayerNorm
and a feed-forward MLP, again with residual, LayerScale, and DropPath.

**Key Parameters:**
- `dim` (int): Dimensionality of token embeddings.
- `num_heads` (int): Number of attention heads.
- `mlp_ratio` (float): Hidden dimension scale factor for the MLP.
- `qkv_bias`, `proj_bias`, `ffn_bias` (bool): Enable bias terms in QKV, projection, and MLP.
- `drop` (float): Dropout rate for projection layers.
- `attn_drop` (float): Dropout rate for attention weights.
- `drop_path` (float): Stochastic depth probability.
- `init_values` (float or None): Initial scale for LayerScale. If None, LayerScale is disabled.
- `act_layer` (Callable): Activation function in the MLP (default: GELU).
- `norm_layer` (Callable): Normalization layer (default: LayerNorm).
- `attn_class` (nn.Module): Attention implementation to use.
- `ffn_layer` (nn.Module): MLP implementation to use.
- `qk_norm` (bool): Apply normalization to Q and K before attention.
- `fused_attn` (bool): Use fused PyTorch scaled_dot_product_attention if available.
- `rope`: Optional `RotaryPositionEmbedding2D` for RoPE in Q/K.

### `NestedTensorBlock`

**Description:**
Extends `Block` to handle a list of tensors (`List[Tensor]`), concatenating them under the hood
and applying a block-diagonal attention mask for independent attention per input. Requires
xFormers for memory-efficient attention.

**Methods:**
- `forward_nested(x_list)`: Takes `List[Tensor]` and returns a list of output tensors.
- `forward(x_or_x_list)`: Dispatches to standard or nested path based on input type.

## Helper Functions

### `drop_add_residual_stochastic_depth(x, residual_func, sample_drop_ratio, pos=None)`

Applies stochastic depth by selecting a random subset of the batch, computing the residual
on that subset, then scaling and scattering it back into the full batch tensor.

### `drop_add_residual_stochastic_depth_list(x_list, residual_func, sample_drop_ratio, scaling_vector=None)`

Batch version of stochastic depth for list inputs, used by `NestedTensorBlock`.

### `get_branges_scales(x, sample_drop_ratio)`

Helper that returns random batch indices and corresponding scaling factor for stochastic depth.

### `add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None)`

Scatters a computed residual back into the original tensor at positions given by `brange`,
optionally scaling per-channel.

### `get_attn_bias_and_cat(x_list, branges=None)`

Constructs a block-diagonal attention bias mask for a list of tensors and concatenates them
for memory-efficient attention. Caches masks for speed.

---

*Generated by developer documentation script.*