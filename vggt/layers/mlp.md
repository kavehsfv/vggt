# mlp.py

This module implements the feed-forward network (MLP) used in transformer blocks. It consists of two linear layers with an activation in between and dropout for regularization.

## Classes

### `Mlp`

**Description:**  
Standard transformer MLP consisting of:
1. Linear projection from `in_features` to `hidden_features`.
2. Activation layer (default: GELU).
3. Dropout.
4. Linear projection from `hidden_features` back to `out_features`.
5. Dropout.

**Constructor Args:**
- `in_features` (`int`): Dimension of the input features.
- `hidden_features` (`int`, optional): Dimension of the hidden layer. Defaults to `in_features`.
- `out_features` (`int`, optional): Dimension of the output features. Defaults to `in_features`.
- `act_layer` (`Callable`): Activation layer. Defaults to `nn.GELU`.
- `drop` (`float`): Dropout probability after each linear + activation.
- `bias` (`bool`): Include bias terms in linear layers.

**Forward:**  
Applies the two-layer MLP to an input tensor of shape `(B, N, in_features)` and returns `(B, N, out_features)`.

**Usage Example:**
```python
from vggt.layers.mlp import Mlp
mlp = Mlp(in_features=1024, hidden_features=4096, drop=0.1)
output = mlp(tokens)  # [B, N, 1024] -> [B, N, 1024]
```

*Generated by developer documentation script.*