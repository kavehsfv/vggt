# patch_embed.py

This module provides the `PatchEmbed` class, which converts an input image into a sequence of patch embeddings for Vision Transformers.

## Functions

### `make_2tuple(x)`

Ensures that an integer or tuple input `x` is converted into a tuple `(x, x)`. Used to normalize arguments for image and patch sizes.

## Classes

### `PatchEmbed`

**Description:**  
Splits a 2D image tensor of shape `(B, C, H, W)` into non-overlapping patches of size `(patch_size, patch_size)` and projects each patch to an embedding vector of dimension `embed_dim`. This is typically used at the beginning of a Vision Transformer to convert image pixels into token embeddings.

**Constructor Args:**
- `img_size` (`int` or `Tuple[int, int]`): Height and width of the input image.
- `patch_size` (`int` or `Tuple[int, int]`): Size of each patch.
- `in_chans` (`int`): Number of input channels (e.g., 3 for RGB).
- `embed_dim` (`int`): Dimension of the output embeddings.
- `norm_layer` (`Callable`, optional): Normalization layer applied after projection. Defaults to Identity.
- `flatten_embedding` (`bool`): If `True`, outputs shape `(B, num_patches, embed_dim)`; if `False`, outputs `(B, H', W', embed_dim)`.

**Attributes:**
- `num_patches` (`int`): Total number of patches.
- `proj` (`nn.Conv2d`): Convolution layer extracting and projecting patches.
- `norm` (`nn.Module`): Normalization layer.

**Forward:**
1. Verifies that `H` and `W` are divisible by `patch_size`.
2. Applies `self.proj` to get a feature map of shape `(B, embed_dim, H', W')`.
3. Flattens spatial dimensions and transposes to `(B, H'*W', embed_dim)`.
4. Applies normalization.
5. Optionally reshapes back to 4D if `flatten_embedding=False`.

**Flops Calculation:**
- `flops()`: Estimates the number of floating-point operations required for the projection and normalization.

*Generated by developer documentation script.*