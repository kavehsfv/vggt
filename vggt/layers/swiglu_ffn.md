# swiglu_ffn.py

This module implements the SwiGLU-based feed-forward network (FFN) variants used in transformer blocks, leveraging gated linear units with SiLU activation.

## Classes

### `SwiGLUFFN`

**Description:**  
Standard SwiGLU FFN that:
1. Projects input from `in_features` to `2 * hidden_features`.
2. Splits channels into `x1`, `x2`.
3. Applies SiLU to `x1` and multiplies element-wise with `x2`.
4. Projects result back to `out_features`.

**Constructor Args:**  
- `in_features` (`int`): Input feature dimension.  
- `hidden_features` (`int`, optional): Hidden layer dimension. Defaults to `in_features`.  
- `out_features` (`int`, optional): Output dimension. Defaults to `in_features`.  
- `bias` (`bool`): Include bias in linear layers.

### `SwiGLUFFNFused`

**Description:**  
Fused variant that adjusts hidden dimension to a multiple of 8 for optimized performance. Falls back to `SwiGLUFFN` if xFormers is unavailable.

**Usage Example:**
```python
from vggt.layers.swiglu_ffn import SwiGLUFFNFused
ffn = SwiGLUFFNFused(in_features=1024, hidden_features=1536)
output = ffn(tokens)  # [B, N, 1024]
```

*Generated by developer documentation script.*