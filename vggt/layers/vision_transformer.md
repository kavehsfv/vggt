# vision_transformer.py

This module implements the `DinoVisionTransformer`, a configurable Vision Transformer (ViT) backbone inspired by DINO-V2. It provides utilities for patch embedding, positional embeddings, transformer blocks, and optional register tokens for downstream heads.

## Functions

### `named_apply(fn: Callable, module: nn.Module, name: str = "", depth_first: bool = True, include_root: bool = False) -> nn.Module`

Recursively applies a given function `fn(module, name)` to all submodules in `module`. Useful for initializing weights or modifying layers after model construction.

## Classes

### `BlockChunk`

**Description:**  
A simple container that groups a list of transformer blocks into a module list and applies them sequentially in the `forward` pass.

### `DinoVisionTransformer`

**Description:**  
End-to-end Vision Transformer with:
- Configurable patch embedding (`PatchEmbed` or custom).
- Learnable class token and optional register tokens.
- Positional embeddings with optional interpolation and anti-aliasing.
- Stack of transformer blocks with stochastic depth and layer scaling.
- Optional block chunking for memory efficiency.
- Optional activation checkpointing for large models.

**Key Constructor Arguments:**
- `img_size` (`int` or tuple): Input image height and width.
- `patch_size` (`int` or tuple): Size of each patch.
- `in_chans` (`int`): Number of input channels.
- `embed_dim` (`int`): Token embedding dimension.
- `depth` (`int`): Number of transformer blocks.
- `num_heads` (`int`): Number of attention heads.
- `mlp_ratio` (`float`): MLP hidden-to-embedding dimension ratio.
- `qkv_bias`, `proj_bias`, `ffn_bias` (`bool`): Enable biases in QKV, projection, and MLP.
- `drop_path_rate` (`float`): Stochastic depth probability.
- `drop_path_uniform` (`bool`): Use uniform drop rate across blocks.
- `init_values` (`float`): Initial layer scale value.
- `embed_layer` (`nn.Module`): Patch embedding class.
- `act_layer` (`nn.Module`): Activation layer for MLP.
- `block_fn` (`nn.Module`): Transformer block class.
- `ffn_layer` (`str`): Type of feed-forward layer (`"mlp"`, `"swiglu"`, `"identity"`).
- `block_chunks` (`int`): Number of block groups for chunking.
- `num_register_tokens` (`int`): Extra tokens appended to the sequence.
- `interpolate_antialias` (`bool`): Anti-aliasing when interpolating positional embeddings.
- `interpolate_offset` (`float`): Offset to avoid floating-point issues.
- `qk_norm` (`bool`): Normalize Q and K before attention.

**Important Methods:**
- `init_weights()`: Initializes positional embeddings, class/register tokens, and block weights.
- `interpolate_pos_encoding(x, w, h)`: Resizes positional embeddings for different resolutions.
- `forward(x)`: Full pipeline: patch embedding → token concatenation → positional embeddings → transformer blocks → final normalization and head.

### Positional Embedding Variants

- `vit_small`, `vit_base`, `vit_large`, `vit_giant2`: Factory functions creating `DinoVisionTransformer` with preset configurations.

---

*Generated by developer documentation script.*